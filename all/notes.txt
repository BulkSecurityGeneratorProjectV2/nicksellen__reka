ok, the problem is this:
- app is running successfully
- deploy new version with errors that won't run
- now we have incorrect code in app dir

possible solutions:
1. deploy into versioned dirs app/1 app/2 etc. and keep a few old ones around
	+ actually this is good. we can remove old one immediately on error. or delete new one.
	- takes up too much space. looks too messy on filesystem.
2. consider the app dir temporary and remove it after it has been deployed, keep the current (and optionally a few older) app archives
	- I like this one. makes it clear you cannot use files from deployment. 
3. deploy from a tmp dir then move into proper place when successful
	- this makes it look like you could keep an absolute filepath reference around but you couldn't. it would change.
	
	
ok I seem to be going with unpacking to versioned dirs. is it a good idea?
downsides:
	- more complicated logic to decide which apps to run on startup. why not just keep around the archives?
		well this allows you to actual use files from the deployment directly.
		
ah, the datadir needs to stay the same though :(
it's only the app dir that needs to have the versioned bit.	


========================

ok for the socket server thing. each application can be deployed with one or more ports.
each port can only be used for one application at a time.
for redeploys that a) are seamless (i.e. no incoming requests are lost) and b) seamless connections
(i.e. connections are persisted across redeploys) - optionally have them drop them on redeploy.

actually I want to unify HTTP/WEBSOCKET/SOCKET totally. as HTTP can be persistant connections anyway.
for doing long-poll stuff. it wor

=========== merge and forEachContent

at the moment MutableData::merge will use Data::forEachContent
the problem with this is that forEachContent skips over empty lists/maps/nulls
as it is only looking for actual content.

I kind of need another thing that does a proper merge.

I've kind of fudged it for now by letting listSet create sparse lists by setting indices above the length.



==================

reka save

oops I just made it so it copies only the root config file not the whole lot.

I need it to copy the zipped up version of it. which has all the stuff in it. doh!

maybe the deploy thing needs to accept the zipped version too, not have it done using generic operations. OR make the api application actually implement the saving restoring kind of bit. just add some operations for that.

reka/persist(/save/etc) 
  - takes a reka zip archive and puts it somewhere under a given identity

reka/restore-all 
  - restores all in a location?
  - or list the location and restore each?


or totally change it and have it so you just drop reka zip configs named with an identity in a particular directory and they get deployed. this way it could be done via the API easily, or just copying things on the filesystem.

moving them out would undeploy them too. it would mean it's hard to get the notifications on API deploy as there is no particular link to it, you'd have to set up notifications to wait for status of it which is kinda clunky. so, no, lets not do it like that.

but it would be nice to be clearer on where we unpack things.

I'd like this thing about having <datadir>/<appid>/<all files for the app>

it would contain the initial deployment files in one directory (perhaps /app) and also all the additional files (perhaps /tmp or /data). maybe these should be different places though. like 
<appdir>/<appid>/<app files>
<datadir>/<appid>/<data files>
<tmpdir>/<appid>/<tmp files>

so you can put them on different files systems as appropriate.

also, when we restore, we just restore the already unpacked application in appdir, not the zipped file. I like this one :) it makes restarting a non destructive operation. and the datadir stays put too. but the tmpdir goes?

the only problem is for distributed systems the datadir is not distributed. or probably not. could be done like that by having the datadir on shared systems. but also would need a local datadir too for things that need to operation on each one. no I think datadir is never distributed. it's only for local state things. distributed operations need to know how to do their thing. either they operate the same on each node so the datadir is created the same each time OR they use another mechanism for distributed operations (like a shared service, etc).

it'd be nice to make a zookeeper module too to show how things can be distributed :) although still, reka itself would need *some* idea of how to distribute things. it would have a few modes, well it would be per module actually:
- one node: module always needs to run on one node (e.g. embedded h2 db, fs?)
- any number: module can be deployed on multiple nodes without co-ordination (e.g. jade)
- high availabliltiy: module can be run on multiple nodes, but only one at once (e.g. I have no idea....)
- distributed: module can be run on multiple nodes and knows how to distribute itself

reka would need to know of all the nodes and how to distribute things. I guess it has a master (like zookeeper kind of master, or hazelcast master) that does the deployments. it would need:
- scheme to detirmine where the modules within an application get deployed
- transport mechanism to send data between nodes when running on different ones
- something to monitor health and reassess decisions if things go wrong